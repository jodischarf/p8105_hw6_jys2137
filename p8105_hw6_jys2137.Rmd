---
title: "p8105_hw6_jys2137"
author: "jys2137"
date: "12/4/2021"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
	dpi = 300,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Problem 1

In this problem, we analyze data gathered to understand the effects of several variables on a child’s birthweight. The `birthweight` dataset consists of roughly 4000 children and includes the following variables:

#### 1.1. Loading and cleaning  the data  

First, we **load** and **clean** the data in preparation for regression analysis. We do this by:

- converting _numeric_ variables to _factor_  (for `babysex`, `frace`, `malform`, and `mrace`)
- converting _imperial_ variable measurements to _metric_ for consistency (weight from pounds to grams, height from inches to centimeters).
- checking for _missing data_

```{r load and clean data}
birthweight_df = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4"),
    malform = as.factor(malform),
    malform = fct_recode(malform, "absent" = "0", "present" = "1"),
    delwt_g = delwt * 453.59237,
    mheight_cm = mheight * 2.54,
    ppwt_g = ppwt * 453.59237,
    wtgain_g = wtgain * 453.59237)

sum(is.na(birthweight_df))
```

Note that in our check for missing data, we can see that there are **no `NA` observations** in the data frame.

#### 1.2. Proposing a regression model  

Now, we develop a regression model for birthweight. The proposed regression model for birthweight is based on a _**hypothesized structure**_ for the factors that underly birthweight (`bwt`):

  * `babysex`: baby’s sex - birthweight could be greater in male vs female;
  * `bhead`: baby’s head circumference at birth (cm); 
  * `blength`: baby’s length at birth (cm) - birthweight could increase with length;
  * `gaweeks`: gestational age in weeks - birthweight could increase with age;
  * `parity`: number of live births prior to this pregnancy - birthweight could decrease with increasing number
  * `smoken`: average number of cigarettes smoked per day during pregnancy 

```{r regression model}
hypoth_model = 
  lm(bwt ~ babysex + bhead + blength + gaweeks + parity + smoken, data = birthweight_df)

hypoth_model %>% 
  broom::tidy() %>% 
  knitr::kable()
```

The code chunk below shows creates a plot of model residuals against fitted values, using `add_predictions` and `add_residuals`.

```{r residual_fitted_ plot}
birthweight_df %>%
  add_residuals(hypoth_model) %>%
  add_predictions(hypoth_model) %>%
  ggplot(aes(x = pred, 
             y = resid)) + 
  geom_point() + 
  geom_smooth(se = FALSE) +
  geom_hline(yintercept = 0, color = "red", size = 1, linetype = 2) +
  labs(
    title = "Model Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals")
```

As shown in the plot of model residuals against fitted values, most residuals are dispersed rather evenly around 0. This cluster indicates a model that is **well-fit**. However, it is important to note that there are a few large differences in the plot. For example, some birthweight fitted values between 500-2000g have a residual greater than 1000g. In addition, the plot illustrates more positive residual values on the lower range of the fitted prediction values, while more negative residuals values are on the higher range of the fitted prediction values. From these observations, we can conclude that the model is **reasonable**, but _not necessarily “optimal”_ (as expected for this problem).

#### 1.3. Comparing regression models

Next, we compare our model to **two others**:

1. One using `blength` and `gaweeks` as predictors (**main effects only**)
2. One using `bhead`, `blength`, and `babysex`and _all interactions_ (including the three-way interaction)

The following code chunk makes these comparisons in terms of the **cross-validated prediction error**, using `crossv_mc` and `dplyr` functions.

```{r comparing_models}
crossv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    main_fx_model = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    hypoth_model = map(train, ~lm(bwt ~ babysex + bhead + blength + gaweeks + parity + smoken, data = .x)),
    interact_model = map(train, ~lm(bwt ~ bhead + blength + babysex + (bhead * blength) + (bhead * babysex) + (blength * babysex) + (bhead * blength * babysex), data = .x))) %>% 
  mutate(
    rmse_main_fx = map2_dbl(main_fx_model, test, rmse),
    rmse_hypoth = map2_dbl(hypoth_model, test, rmse),
    rmse_interact = map2_dbl(interact_model, test, rmse)) 

crossv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
    geom_violin(alpha = 0.80) +
    labs(
        title = "Cross Validation of Comparison Models",
        x = "Model",
        y = "Root Mean Squared Error (RMSE)") +
    scale_x_discrete(labels = c(
      "main_fx" = "Main Effects Model", 
      "hypoth" = "Proposed Model",
      "interact" = "Interaction Model")) +
    theme(legend.position = "none")
```

The violin plot of RMSE illustrates that our **proposed model** actually has the lowest root mean squared error in comparison with the other two models, making it the best fit model among the three. Note that it only has a _very slightly_ lower RMSE than the **interaction model** (`bhead`, `blength`, and `babysex`and _all interactions_), while it is a much better fit than the **main effects model** (with `blength` and `gaweeks`), which had the highest root mean squared error. 

This suggests that the best predictors to use are the _baby's sex_, _head circumference_, _length_, _gestational age_, as well as _parity_ and _maternal smoking while pregnant_.

### Problem 2

For this problem, we explore boostrapping by using weather data from NOAA that involves the `minimum` and `maximum` temperatures for Central Park in 2017.

#### 2.1. Loading the data

The code chunk below loads the weather dataset.

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

#### 2.2. Bootstrap
Now, we focus on a simple linear regression and are interested in the distribution of two quantities estimated from these data:

1. $\hat{r}^2$
2. $\log(\beta_0 * \beta1)$

We will use 5000 bootstrap samples from the `weather_df` data frame. For each sample, we fit a simple linear regression with `tmax` as the response and `tmin` as the predictor.

The chunk below sets the bootstraps up. We use `broom::glance()` for getting $\hat{r}^2$ and `broom::tidy()` for getting $\log(\beta_0 * \beta_1)$ from the fitted regression.

```{r bootstrap setup, warning = FALSE}
set.seed(123)

boot_strap =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    glance = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results, glance) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate) %>% 
  rename(
    beta0 = `(Intercept)`,
    beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  select(r.squared, log_b0b1)
```

#### 2.3. Estimate $\hat{r}^2$

To plot the distribution of the estimates of $\hat{r}^2$, we use the following code chunk. 

```{r boostrap_r_sq}
boot_strap %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density(alpha = 0.2) +
  stat_function(fun = dnorm,
                args = with(boot_strap, c(mean = mean(r.squared), sd = sd(r.squared))),
                color = "red",
                size = 1.5,
                alpha = .7) +
  theme(legend.position = "none") +
  labs(
        title = "Distribution of" ~R^2, 
        subtitle = "Based on 5000 bootstrap samples of a linear model",
        x = "Estimate",
        y = "Density")
```

The estimates for **$\hat{r}^2$** looks to be roughly **normally distributed**. However, it is important to note that the estimate is high, as the values are distributed closer to the upper bound of 1. It could be argued that there is a very slight left-skew in the distribution, but this is likely due to the high estimate of $\hat{r}^2$. This suggests that a vast majority of the variation in `tmax` can be explained by `tmin`.

#### 2.4. Estimate for $\log(\beta_0 * \beta_1)$

Similarly, we plot the distribution of the estimates of $\log(\beta_0 * \beta_1)$, using the code chunk below. Note that we need to do some more data wrangling for this plot.

```{r boostrap_log}
boot_strap %>% 
  ggplot(aes(x = log_b0b1)) + 
  geom_density(alpha = 0.2) +
  stat_function(fun = dnorm,
                args = with(boot_strap, c(mean = mean(log_b0b1), sd = sd(log_b0b1))),
                color = "blue",
                size = 1.5,
                alpha = .7) +
  theme(legend.position = "none") +
  labs(
        title = "Distribution of" ~log(hat(beta)[0] %*% hat(beta)[1]),
        subtitle = "Based on 5000 bootstrap samples of a linear model",
        x = "Estimate",
        y = "Density")
```

Similar to the **$\hat{r}^2$** estimate, the distribution for **$\log(\beta_0 * \beta_1)$** is also roughly **normally distributed**. However, the plot shows that the **peak** of this distribution is not exactly normal as it has a _very small dip_ at the top of the distribution.

#### 2.5. 95% Confidence Intervals

Using the 5000 bootstrap estimates, we can identify the 2.5% and 97.5% quantiles to provide a **95% confidence interval** for **$\hat{r}^2$** and **$\log(\beta_0 * \beta_1)$**.

- The _95% confidence interval_ of **`R-squared`** is (`r round(quantile(pull(boot_strap, r.squared), probs = c(0.025,0.975)), digits = 2)`).

- The _95% confidence interval_ of **`log(beta0 x beta1)`** is (`r round(quantile(pull(boot_strap, log_b0b1), probs = c(0.025,0.975)), digits = 2)`).  

The code chunk below produces a useful table with point estimates and confidence intervals or $\hat{r}^2$ and $\log(\beta_0 * \beta_1)$.

```{r estimate_ci_table}
# Get estimate and confidence interval for each term
tibble(term = c("R-Squared", "Log Product"),
       Estimate = c(mean(pull(boot_strap, r.squared)),
                    mean(pull(boot_strap, log_b0b1))),
       "Lower CI" = c(quantile(pull(boot_strap, r.squared), 0.025),
                    quantile(pull(boot_strap, log_b0b1), 0.025)),
       "Upper CI" = c(quantile(pull(boot_strap, r.squared), 0.975),
                    quantile(pull(boot_strap, log_b0b1), 0.975))) %>%
  knitr::kable()
```