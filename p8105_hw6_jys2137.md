p8105\_hw6\_jys2137
================
jys2137
12/4/2021

### Problem 1

In this problem, we analyze data gathered to understand the effects of
several variables on a child’s birthweight.

#### 1.1. Loading and cleaning the data

First, we **load** and **clean** the data in preparation for regression
analysis. We do this by:

-   converting *numeric* variables to *factor* (for `babysex`, `frace`,
    `malform`, and `mrace`)
-   converting *imperial* variable measurements to *metric* for
    consistency (weight from pounds to grams, height from inches to
    centimeters).
-   checking for *missing data*

``` r
birthweight_df = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4"),
    malform = as.factor(malform),
    malform = fct_recode(malform, "absent" = "0", "present" = "1"),
    delwt_g = delwt * 453.59237,
    mheight_cm = mheight * 2.54,
    ppwt_g = ppwt * 453.59237,
    wtgain_g = wtgain * 453.59237)
```

    ## Rows: 4342 Columns: 20

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
sum(is.na(birthweight_df))
```

    ## [1] 0

Note that in our check for missing data, we can see that there are **no
missing observations** (`NA`) in the data frame.

#### 1.2. Proposing a regression model

Now, we develop a regression model for birthweight. The proposed
regression model for birthweight is based on a ***hypothesized
structure*** for the factors that underly birthweight (`bwt`):

-   `babysex`: baby’s sex - birthweight could be greater in male vs
    female;
-   `bhead`: baby’s head circumference at birth (cm);
-   `blength`: baby’s length at birth (cm) - birthweight could increase
    with length;
-   `gaweeks`: gestational age in weeks - birthweight could increase
    with age;
-   `parity`: number of live births prior to this pregnancy -
    birthweight could decrease with increasing number
-   `smoken`: average number of cigarettes smoked per day during
    pregnancy

``` r
hypoth_model = 
  lm(bwt ~ babysex + bhead + blength + gaweeks + parity + smoken, data = birthweight_df)

hypoth_model %>% 
  broom::tidy() %>% 
  knitr::kable()
```

| term          |     estimate |  std.error |  statistic |   p.value |
|:--------------|-------------:|-----------:|-----------:|----------:|
| (Intercept)   | -6248.266322 | 98.1489432 | -63.661066 | 0.0000000 |
| babysexfemale |    31.656360 |  8.8379620 |   3.581862 | 0.0003449 |
| bhead         |   140.297267 |  3.5632792 |  39.373077 | 0.0000000 |
| blength       |    81.489997 |  2.0808687 |  39.161527 | 0.0000000 |
| gaweeks       |    14.687050 |  1.5192664 |   9.667199 | 0.0000000 |
| parity        |   104.235409 | 42.2261679 |   2.468503 | 0.0136063 |
| smoken        |    -1.765926 |  0.5878128 |  -3.004233 | 0.0026777 |

The code chunk below shows creates a plot of model residuals against
fitted values, using `add_predictions` and `add_residuals`.

``` r
birthweight_df %>%
  add_residuals(hypoth_model) %>%
  add_predictions(hypoth_model) %>%
  ggplot(aes(x = pred, 
             y = resid)) + 
  geom_point() + 
  geom_smooth(se = FALSE) +
  geom_hline(yintercept = 0, color = "red", size = 1, linetype = 2) +
  labs(
    title = "Model Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals")
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw6_jys2137_files/figure-gfm/residual_fitted_ plot-1.png" width="90%" />

As shown in the plot of model residuals against fitted values, most
residuals are dispersed rather evenly around 0. This cluster indicates a
model that is **well-fit**. However, it is important to note that there
are a few largely outlying points in the plot. For example, some
birthweight fitted values between 500-2000g have a residual greater than
1000g. In addition, the plot illustrates more positive residual values
on the lower range of the fitted prediction values, while more negative
residuals values are on the higher range of the fitted prediction
values. From these observations, we can conclude that the model is
**reasonable**, but *not necessarily “optimal”* (as expected for this
problem).

#### 1.3. Comparing regression models

Next, we compare our model to **two others**:

1.  One using `blength` and `gaweeks` as predictors (**main effects
    only**)
2.  One using `bhead`, `blength`, and `babysex`and *all interactions*
    (including the three-way interaction)

The following code chunk makes these comparisons in terms of the
**cross-validated prediction error**, using `crossv_mc` and `dplyr`
functions.

``` r
crossv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    main_fx_model = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    hypoth_model = map(train, ~lm(bwt ~ babysex + bhead + blength + gaweeks + parity + smoken, data = .x)),
    interact_model = map(train, ~lm(bwt ~ bhead + blength + babysex + (bhead * blength) + (bhead * babysex) + (blength * babysex) + (bhead * blength * babysex), data = .x))) %>% 
  mutate(
    rmse_main_fx = map2_dbl(main_fx_model, test, rmse),
    rmse_hypoth = map2_dbl(hypoth_model, test, rmse),
    rmse_interact = map2_dbl(interact_model, test, rmse)) 

crossv_df %>% 
  dplyr::select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
    geom_violin(alpha = 0.80) +
    labs(
        title = "Cross Validation of Comparison Models",
        x = "Model",
        y = "Root Mean Squared Error (RMSE)") +
    scale_x_discrete(labels = c(
      "main_fx" = "Main Effects Model", 
      "hypoth" = "Proposed Model",
      "interact" = "Interaction Model")) +
    theme(legend.position = "none")
```

<img src="p8105_hw6_jys2137_files/figure-gfm/comparing_models-1.png" width="90%" />

The violin plot of RMSE illustrates that our **proposed model** actually
has the lowest root mean squared error in comparison with the other two
models, making it the best fit model among the three. Note that it only
has a *very slightly* lower RMSE than the **interaction model**
(`bhead`, `blength`, and `babysex`and *all interactions*), while it is a
much better fit than the **main effects model** (with `blength` and
`gaweeks`) which had the highest root mean squared error.

This suggests that good predictors to use are the *baby’s sex*, *head
circumference*, *length*, *gestational age*, as well as *parity* and
*maternal smoking while pregnant*.

### Problem 2

For this problem, we explore boostrapping by using weather data from
NOAA that involves the `minimum` and `maximum` temperatures for Central
Park in 2017.

#### 2.1. Loading the data

The code chunk below loads the weather dataset.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2021-12-04 01:44:20 (7.616)

    ## file min/max dates: 1869-01-01 / 2021-12-31

#### 2.2. Bootstrap

Now, we focus on a simple linear regression and are interested in the
distribution of two quantities estimated from these data:

1.  *r̂*<sup>2</sup>
2.  log (*β*<sub>0</sub> \* *β*1)

We will use 5000 bootstrap samples from the `weather_df` data frame. For
each sample, we fit a simple linear regression with `tmax` as the
response and `tmin` as the predictor.

The chunk below sets the bootstraps up. We use `broom::glance()` for
getting *r̂*<sup>2</sup> and `broom::tidy()` for getting
log (*β*<sub>0</sub> \* *β*<sub>1</sub>) from the fitted regression.

``` r
set.seed(123)

boot_strap =
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>%
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy),
    glance = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results, glance) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate) %>% 
  rename(
    beta0 = `(Intercept)`,
    beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  select(r.squared, log_b0b1)
```

#### 2.3. Estimate *r̂*<sup>2</sup>

To plot the distribution of the estimates of *r̂*<sup>2</sup>, we use the
following code chunk.

``` r
boot_strap %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density(alpha = 0.2) +
  stat_function(fun = dnorm,
                args = with(boot_strap, c(mean = mean(r.squared), sd = sd(r.squared))),
                color = "red",
                size = 1.5,
                alpha = .7) +
  theme(legend.position = "none") +
  labs(
        title = "Distribution of" ~R^2, 
        subtitle = "Based on 5000 bootstrap samples of a linear model",
        x = "Estimate",
        y = "Density")
```

<img src="p8105_hw6_jys2137_files/figure-gfm/boostrap_r_sq-1.png" width="90%" />

The estimates for ***r̂*<sup>2</sup>** looks to be roughly **normally
distributed**. However, it is important to note that the estimate is
high, as the values are distributed closer to the upper bound of 1. It
could be argued that there is a very slight left-skew in the
distribution, but this is likely due to the high estimate of
*r̂*<sup>2</sup>. This suggests that a vast majority of the variation in
`tmax` can be explained by `tmin`.

#### 2.4. Estimate for log (*β*<sub>0</sub> \* *β*<sub>1</sub>)

Similarly, we plot the distribution of the estimates of
log (*β*<sub>0</sub> \* *β*<sub>1</sub>), using the code chunk below.
Note that we need to do some more data wrangling for this plot.

``` r
boot_strap %>% 
  ggplot(aes(x = log_b0b1)) + 
  geom_density(alpha = 0.2) +
  stat_function(fun = dnorm,
                args = with(boot_strap, c(mean = mean(log_b0b1), sd = sd(log_b0b1))),
                color = "blue",
                size = 1.5,
                alpha = .7) +
  theme(legend.position = "none") +
  labs(
        title = "Distribution of" ~log(hat(beta)[0] %*% hat(beta)[1]),
        subtitle = "Based on 5000 bootstrap samples of a linear model",
        x = "Estimate",
        y = "Density")
```

<img src="p8105_hw6_jys2137_files/figure-gfm/boostrap_log-1.png" width="90%" />

Similar to the ***r̂*<sup>2</sup>** estimate, the distribution for
**log (*β*<sub>0</sub> \* *β*<sub>1</sub>)** is also roughly **normally
distributed**. However, the plot shows that the **peak** of this
distribution is not exactly normal as it has a *very small dip* at the
top of the distribution.

#### 2.5. 95% Confidence Intervals

Using the 5000 bootstrap estimates, we can identify the 2.5% and 97.5%
quantiles to provide a **95% confidence interval** for
***r̂*<sup>2</sup>** and **log (*β*<sub>0</sub> \* *β*<sub>1</sub>)**.

-   The *95% confidence interval* of **`R-squared`** is (0.89, 0.93).

-   The *95% confidence interval* of **`log(beta0 x beta1)`** is (1.96,
    2.06).

The code chunk below produces a useful table with point estimates and
confidence intervals or *r̂*<sup>2</sup> and
log (*β*<sub>0</sub> \* *β*<sub>1</sub>).

``` r
# Get estimate and confidence interval for each term
tibble(term = c("R-Squared", "Log Product"),
       Estimate = c(mean(pull(boot_strap, r.squared)),
                    mean(pull(boot_strap, log_b0b1))),
       "Lower CI" = c(quantile(pull(boot_strap, r.squared), 0.025),
                    quantile(pull(boot_strap, log_b0b1), 0.025)),
       "Upper CI" = c(quantile(pull(boot_strap, r.squared), 0.975),
                    quantile(pull(boot_strap, log_b0b1), 0.975))) %>%
  knitr::kable()
```

| term        |  Estimate |  Lower CI |  Upper CI |
|:------------|----------:|----------:|----------:|
| R-Squared   | 0.9115107 | 0.8945701 | 0.9271042 |
| Log Product | 2.0129216 | 1.9641656 | 2.0583636 |
